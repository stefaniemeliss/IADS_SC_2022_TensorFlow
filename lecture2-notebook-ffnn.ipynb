{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Lecture 2 notebook part 1: Build a Feedforward neural network\n",
    "## Introduction to TensorFlow and Deep Learning\n",
    "\n",
    "## IADS Summer School, 1st August 2022\n",
    "\n",
    "### Dr Michael Fairbank, University of Essex, UK\n",
    "\n",
    "- Email: m.fairbank@essex.ac.uk\n",
    "- This is a Jupyter Notebook to accompany Lecture 2 of the course"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Feedforward algorithm (Exercise 1)\n",
    "\n",
    "- Complete the code below to implement a 2-2-1 (1 hidden layer of just 2 nodes) feedforward network. \n",
    "- If successful, you should see the output value 0.50787264.\n",
    "\n",
    "\n",
    "\n",
    "Exercise 1: (Template code is on next slide and in lecture2-notebook-ffnn)\n",
    "1. Create a 2-2-1 feedforward network. (One hidden layer, of just 2 nodes).\n",
    "2. Use sigmoid activation functions at each layer. tf.sigmoid(â€¦.)\n",
    "3. Randomise weights and biases.\n",
    "4. Then evaluate your network with input vector x = [1, 1]. Check this gives an output of y = [0.50787264]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.5078727]]\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "x=tf.constant([[1,1]], tf.float32) # our input vector\n",
    "\n",
    "# Build our random weight and bias matrices, of appropriate shapes\n",
    "W1 = tf.Variable(tf.random.truncated_normal([2,2], stddev=0.1, seed=1)) # weight matrix from input to hidden layer\n",
    "b1 = tf.Variable(tf.random.truncated_normal([1,2], stddev=0.1, seed=2)) # bias (i.e., threshold) matrix from input to hidden layer\n",
    "W2 = tf.Variable(tf.random.truncated_normal([2,1], stddev=0.1, seed=3)) # weight matrix from hidden to output layer\n",
    "b2 = tf.Variable(tf.random.truncated_normal([1,1], stddev=0.1, seed=4)) # bias (i.e., threshold) matrix from input to hidden layer\n",
    "# define our feed-forward neural network here:\n",
    "def run_network(x):\n",
    "    h1=tf.sigmoid(tf.matmul(x,W1) + b1) # execute matrix multiplication\n",
    "    y=tf.sigmoid(tf.matmul(h1,W2) + b2) # execute matrix multiplication\n",
    "    return y\n",
    "    \n",
    "print(run_network(x).numpy()) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exercise 2\n",
    "- In the code block below, redefine the variable x to represent 4 input patterns as described in the lecture slides.\n",
    "- Rerun your run_network function (no need to redefine that function though) and check it outputs 0.509233, 0.509222, 0.507883, 0.507872 \n",
    "- Due to strange random-number generator behaviour with Jupyter+Tensorflow, Please reset the jupyter kernel just before running your code block below if you want to replicate the values 0.509233, 0.509222, 0.507883, 0.507872 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.50923383]\n",
      " [0.50922257]\n",
      " [0.50788385]\n",
      " [0.5078727 ]]\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "x=tf.constant([[0,0],[0,1],[1,0],[1,1]], tf.float32) # our input vector\n",
    "\n",
    "# Build our random weight and bias matrices, of appropriate shapes\n",
    "W1 = tf.Variable(tf.random.truncated_normal([2,2], stddev=0.1, seed=1)) # weight matrix from input to hidden layer\n",
    "b1 = tf.Variable(tf.random.truncated_normal([1,2], stddev=0.1, seed=2)) # bias (i.e., threshold) matrix from input to hidden layer\n",
    "W2 = tf.Variable(tf.random.truncated_normal([2,1], stddev=0.1, seed=3)) # weight matrix from hidden to output layer\n",
    "b2 = tf.Variable(tf.random.truncated_normal([1,1], stddev=0.1, seed=4)) # bias (i.e., threshold) matrix from input to hidden layer\n",
    "# define our feed-forward neural network here:\n",
    "def run_network(x):\n",
    "    h1=tf.sigmoid(tf.matmul(x,W1) + b1) # execute matrix multiplication\n",
    "    y=tf.sigmoid(tf.matmul(h1,W2) + b2) # execute matrix multiplication\n",
    "    return y\n",
    "    \n",
    "print(run_network(x).numpy()) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train your network in a loop (Exercise 3)\n",
    "\n",
    "- Fill in the TODOs in the code block below to train the network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration  0  loss 1.0001175\n",
      "iteration  1000  loss 1.000001\n",
      "iteration  2000  loss 1.0000005\n",
      "iteration  3000  loss 1.0000001\n",
      "iteration  4000  loss 0.99999976\n",
      "iteration  5000  loss 0.99999946\n",
      "iteration  6000  loss 0.9999988\n",
      "iteration  7000  loss 0.99999833\n",
      "iteration  8000  loss 0.9999969\n",
      "iteration  9000  loss 0.99999374\n",
      "iteration  10000  loss 0.9999815\n",
      "iteration  11000  loss 0.99982667\n",
      "iteration  12000  loss 0.04408118\n",
      "iteration  13000  loss 0.005059277\n",
      "iteration  14000  loss 0.0025747474\n",
      "iteration  15000  loss 0.0017117967\n",
      "iteration  16000  loss 0.0012774467\n",
      "iteration  17000  loss 0.0010169512\n",
      "iteration  18000  loss 0.0008437152\n",
      "iteration  19000  loss 0.0007203496\n",
      "[[0.01252078]\n",
      " [0.9882424 ]\n",
      " [0.98561877]\n",
      " [0.01124119]]\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "y_labels = tf.constant([[0],[1],[1],[0]], tf.float32) # TODO: Define your constant 4*1 tensor y_labels based on XOR truth table\n",
    "\n",
    "def calc_loss(): # TODO: define loss function\n",
    "    y=run_network(x) \n",
    "    deltas=tf.subtract(y, y_labels)\n",
    "    squared_deltas=tf.square(deltas)\n",
    "    loss=tf.reduce_sum(squared_deltas)\n",
    "    return loss\n",
    "\n",
    "optimizer = tf.keras.optimizers.SGD(0.5) # 0.5 --> learning rate eta\n",
    "for i in range(20000): #TODO: Train your network in a loop\n",
    "    optimizer.minimize(calc_loss, [W1,b1,W2,b2])\n",
    "    if (i%1000)==0: # only print output of every 1000 iterations\n",
    "        print(\"iteration \",i,\" loss\", calc_loss().numpy())       \n",
    "\n",
    "print(run_network(x).numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.01183246]]\n"
     ]
    }
   ],
   "source": [
    "# Running the neural network on new data\n",
    "print(run_network ( [[0.5,0.5]]).numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Rewrite your code to use the keras layers (Optional Exercise 4)\n",
    "\n",
    "- Add some code into the block below to build a network from scratch, using Keras layers, and train it.\n",
    "- Note, you might need to run the training loop several times from freshly randomised weights to get it to converge."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO- Add code for Exercise 4 here:\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
